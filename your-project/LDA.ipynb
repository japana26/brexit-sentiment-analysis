{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOPIC ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/nlp/topic-modeling-visualization-how-to-present-results-lda-models/\n",
    "    \n",
    "https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import corpus\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "\n",
    "from nltk.stem import PorterStemmer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    stop_words = stopwords.words('english')\n",
    "    stop_words += ['from', 'subject', 're', 'edu', 'use','user', 'com', 'co', 'con', 'be', 'else', 'http', 'would','send', \n",
    "                   'do', 'try', 'tell', 'go', 'get', 'can', 'think', 'know', 'give', 'ask', \n",
    "               'next', 'find', 're']\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word.strip() not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out\n",
    "\n",
    "def stem_and_lemmatize(tweet):\n",
    "    tweet = ' '.join(tweet)\n",
    "    stem = PorterStemmer().stem(tweet)\n",
    "    return WordNetLemmatizer().lemmatize(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = pd.read_csv('cdf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "none             0.901269\n",
       "Boris Johnson    0.053743\n",
       "Theresa May      0.044988\n",
       "Name: PM, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf['PM'].value_counts()/len(cdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305304, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing dataset from 300k to 30k (removing tweets that do not refer to either Boris or Theresa May)\n",
    "df = cdf.drop(cdf[(cdf.PM == 'none')].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30143, 7)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping nan values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Boris Johnson    16407\n",
       "Theresa May      13735\n",
       "Name: PM, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PM'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping duplicates\n",
    "df.drop_duplicates(subset='TEXT', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset by months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jan = df[(df.MONTH_STR == 'Jan')]\n",
    "df_fev = df[(df.MONTH_STR == 'Fev')]\n",
    "df_march = df[(df.MONTH_STR == 'Mar')]\n",
    "df_april = df[(df.MONTH_STR == 'Apr')]\n",
    "df_may = df[(df.MONTH_STR == 'May')]\n",
    "df_june = df[(df.MONTH_STR == 'Jun')] \n",
    "df_july = df[(df.MONTH_STR == 'Jul')]\n",
    "df_aug = df[(df.MONTH_STR == 'Aug')] \n",
    "df_sep = df[(df.MONTH_STR == 'Sep')] \n",
    "df_oct = df[(df.MONTH_STR == 'Oct')] \n",
    "df_nov = df[(df.MONTH_STR == 'Nov')] \n",
    "df_dec = df[(df.MONTH_STR == 'Dec')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_march = pd.concat([df_jan,df_fev,df_march,df_april])\n",
    "df_june = pd.concat([df_may,df_june,df_july, df_aug])\n",
    "df_sep = pd.concat([df_sep,df_oct,df_nov, df_dec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apr    3604\n",
       "Dec    3149\n",
       "Oct    3052\n",
       "Mar    2964\n",
       "Jan    2741\n",
       "Nov    2415\n",
       "Sep    2115\n",
       "Jul    2031\n",
       "Feb    1879\n",
       "Jun    1555\n",
       "Aug    1479\n",
       "May    1182\n",
       "Name: MONTH_STR, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.MONTH_STR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming the series into lists for text processing\n",
    "df_march = df_march['TWEET_CLEANED'].tolist()\n",
    "df_july = df_july['TWEET_CLEANED'].tolist()\n",
    "df_nov = df_nov['TWEET_CLEANED'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dictionary out of bigrams\n",
    "\n",
    "Biagrams are 2 words frequently appearing together. This way I will be able to observe which words appear frequently before or after brexit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### January - April Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df_march, min_count=1, threshold=1) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Stop Words\n",
    "data_words_nostops = remove_stopwords(df_march)\n",
    "#creating bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "#Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_m = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uphold', 'rule', 'crime', 'laughable', 'pmqs', 'brexitshamble']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_m[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary from the unique bigrams \n",
    "id2word_m = corpora.Dictionary(data_lemmatized_m)\n",
    "id2word_m.filter_extremes(no_below=10, no_above=0.2) #excluding tokens that ocurred in less than 10 tweets and bigrams that occurred in more than 50% of the tweets\n",
    "# Rebuild corpus based on the dictionary\n",
    "texts_m = data_lemmatized_m\n",
    "for i in texts_m:\n",
    "    for k in i:\n",
    "        remove = ['from', 'subject', 're', 'edu', 'use','user', 'com', 'co', 'con', 'be', 'else', 'http', 'would','send', \n",
    "                   'do', 'try', 'tell', 'go', 'get', 'can', 'think', 'know', 'give', 'ask', 's','mean','take','name','local',\n",
    "                  'next', 'find', 're', 'semi','week', 'day', 'want', 'mail', 'run', 'ree', 'other', 'many', 'day', 'year']\n",
    "        if k in remove:\n",
    "            i.remove(k)\n",
    "# Term Document Frequency\n",
    "corpus_m = [id2word_m.doc2bow(text) for text in texts_m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### May - August Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df_july, min_count=50, threshold=1) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Stop Words\n",
    "data_words_nostops = remove_stopwords(df_july)\n",
    "#creating bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "#Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_j = lemmatization(data_words_bigrams, allowed_postags=['NOUN','ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tell', 'offer', 'democracy']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_j[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary from the unique bigrams \n",
    "id2word_j = corpora.Dictionary(data_lemmatized_j)\n",
    "id2word_j.filter_extremes(no_below=10, no_above=0.2) #excluding tokens that ocurred in less than 10 tweets and bigrams that occurred in more than 50% of the tweets\n",
    "# Rebuild corpus based on the dictionary\n",
    "texts_j = data_lemmatized_j\n",
    "for i in texts_j:\n",
    "    for k in i:\n",
    "        remove = ['from', 'subject', 're', 'edu', 'use','user', 'com', 'co', 'con', 'be', 'else', 'http', 'would','send', \n",
    "                   'do', 'try', 'tell', 'go', 'get', 'can', 'think', 'know', 'give', 'ask', 's','mean','take','name','local',\n",
    "                  'next', 'find', 're', 'semi','week', 'day', 'want', 'mail', 'run', 'ree', 'other', 'many', 'day', 'year']\n",
    "        if k in remove:\n",
    "            i.remove(k)\n",
    "# Term Document Frequency\n",
    "corpus_j = [id2word_j.doc2bow(text) for text in texts_j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### September - December Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(df_nov, min_count=50, threshold=1) # higher threshold fewer phrases.\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove Stop Words\n",
    "data_words_nostops = remove_stopwords(df_nov)\n",
    "#creating bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "#Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized_n = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'VERB', 'ADJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corbyn', 'supporter', 'check', 'remainer', 'need', 'hold', 'nose', 'good', 'judgement', 'stop', 'get', 'majority', 'ensure', 'deal', 'brexit', 'good', 'site', 'compare', 'tactical', 'voting', 'sit']\n"
     ]
    }
   ],
   "source": [
    "print(data_lemmatized_n[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary from the unique bigrams \n",
    "id2word_n = corpora.Dictionary(data_lemmatized_n)\n",
    "id2word_n.filter_extremes(no_below=10, no_above=0.2) #excluding tokens that ocurred in less than 10 tweets and bigrams that occurred in more than 50% of the tweets\n",
    "# Rebuild corpus based on the dictionary\n",
    "texts_n = data_lemmatized_n\n",
    "for i in texts_n:\n",
    "    for k in i:\n",
    "        remove = ['from', 'subject', 're', 'edu', 'use','user', 'com', 'co', 'con', 'be', 'else', 'http', 'would','send', \n",
    "                   'do', 'try', 'tell', 'go', 'get', 'can', 'think', 'know', 'give', 'ask', 's','mean','take','name','local','next', 'find', 're', 'semi','week', 'day', 'want', 'mail', 'run', 'ree', 'other', 'many', 'day', 'year']\n",
    "        if k in remove:\n",
    "            i.remove(k)\n",
    "# Term Document Frequency\n",
    "corpus_n = [id2word_n.doc2bow(text) for text in texts_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF\n",
    "\n",
    "A problem with this approach is that highly frequent words start to dominate in the document, but may not be representative to the model as less-frequent words. One way to fix this is to measure how unique (or how infrequent) a word is across all documents (or tweets), which is called the “inverse document frequency” or IDF. By introducing IDF, frequent words that are also frequent across all documents get penalized with less weight.\n",
    "\n",
    "https://towardsdatascience.com/topic-modeling-of-2019-hr-tech-conference-twitter-d16cf75895b6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus_m)\n",
    "tfidf_corpus_m = tfidf[corpus_m]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus_j)\n",
    "tfidf_corpus_j = tfidf[corpus_j]\n",
    "\n",
    "tfidf = models.TfidfModel(corpus_n)\n",
    "tfidf_corpus_n = tfidf[corpus_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling Via LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jan - April"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_m = gensim.models.ldamodel.LdaModel(corpus=tfidf_corpus_m,\n",
    "                                           id2word=id2word_m,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.028*\"talk\" + 0.025*\"delay\" + 0.020*\"sign\" + 0.020*\"happen\" + 0.018*\"lie\" '\n",
      "  '+ 0.017*\"help\" + 0.015*\"turn\" + 0.015*\"promise\" + 0.014*\"control\" + '\n",
      "  '0.014*\"hope\"'),\n",
      " (1,\n",
      "  '0.046*\"vote\" + 0.038*\"polling\" + 0.027*\"remain\" + 0.026*\"labour\" + '\n",
      "  '0.021*\"need\" + 0.020*\"face\" + 0.017*\"today\" + 0.017*\"good\" + '\n",
      "  '0.017*\"government\" + 0.015*\"call\"'),\n",
      " (2,\n",
      "  '0.036*\"claim\" + 0.033*\"tory\" + 0.026*\"could\" + 0.022*\"election\" + '\n",
      "  '0.021*\"leader\" + 0.016*\"month\" + 0.014*\"corbyn\" + 0.014*\"news\" + '\n",
      "  '0.013*\"stay\" + 0.012*\"right\"'),\n",
      " (3,\n",
      "  '0.026*\"conservative\" + 0.025*\"brexit\" + 0.023*\"deal\" + 0.019*\"say\" + '\n",
      "  '0.019*\"people\" + 0.017*\"leave\" + 0.016*\"make\" + 0.015*\"deliver\" + '\n",
      "  '0.015*\"party\" + 0.014*\"time\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 4 topics\n",
    "pprint(lda_model_m.print_topics())\n",
    "doc_lda_m = lda_model_m[corpus_m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: talk|delay|sign|happen|lie|help|turn|promise|control|hope\n",
      "Topic: 1 \n",
      "Words: vote|polling|remain|labour|need|face|today|good|government|call\n",
      "Topic: 2 \n",
      "Words: claim|tory|could|election|leader|month|corbyn|news|stay|right\n",
      "Topic: 3 \n",
      "Words: conservative|brexit|deal|say|people|leave|make|deliver|party|time\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_m.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### May - August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_j = gensim.models.ldamodel.LdaModel(corpus=tfidf_corpus_j,\n",
    "                                           id2word=id2word_j,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.050*\"deal\" + 0.049*\"brexit\" + 0.038*\"say\" + 0.028*\"people\" + '\n",
      "  '0.026*\"leave\" + 0.021*\"time\" + 0.020*\"election\" + 0.019*\"need\" + '\n",
      "  '0.018*\"call\" + 0.017*\"promise\"'),\n",
      " (1,\n",
      "  '0.094*\"deliver\" + 0.044*\"stop\" + 0.041*\"conservative\" + 0.040*\"come\" + '\n",
      "  '0.028*\"good\" + 0.028*\"theresa\" + 0.026*\"corbyn\" + 0.021*\"face\" + '\n",
      "  '0.018*\"show\" + 0.018*\"also\"'),\n",
      " (2,\n",
      "  '0.037*\"make\" + 0.035*\"tory\" + 0.029*\"borisjohnson\" + 0.028*\"back\" + '\n",
      "  '0.027*\"even\" + 0.027*\"plan\" + 0.026*\"country\" + 0.025*\"remain\" + '\n",
      "  '0.024*\"could\" + 0.024*\"become\"'),\n",
      " (3,\n",
      "  '0.051*\"vote\" + 0.033*\"labour\" + 0.033*\"happen\" + 0.031*\"poll\" + '\n",
      "  '0.025*\"party\" + 0.024*\"voter\" + 0.024*\"work\" + 0.024*\"look\" + 0.023*\"talk\" '\n",
      "  '+ 0.023*\"well\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model_j.print_topics())\n",
    "doc_lda_j = lda_model_j[corpus_j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: deal|brexit|say|people|leave|time|election|need|call|promise\n",
      "Topic: 1 \n",
      "Words: deliver|stop|conservative|come|good|theresa|corbyn|face|show|also\n",
      "Topic: 2 \n",
      "Words: make|tory|borisjohnson|back|even|plan|country|remain|could|become\n",
      "Topic: 3 \n",
      "Words: vote|labour|happen|poll|party|voter|work|look|talk|well\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_j.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### September - December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_n = gensim.models.ldamodel.LdaModel(corpus=tfidf_corpus_n,\n",
    "                                           id2word=id2word_n,\n",
    "                                           num_topics=4, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=200,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.033*\"say\" + 0.033*\"election\" + 0.032*\"conservative\" + 0.027*\"need\" + '\n",
      "  '0.025*\"corbyn\" + 0.024*\"make\" + 0.023*\"deliver\" + 0.022*\"remain\" + '\n",
      "  '0.020*\"campaign\" + 0.020*\"farage\"'),\n",
      " (1,\n",
      "  '0.031*\"keep\" + 0.027*\"full\" + 0.024*\"fact\" + 0.023*\"call\" + 0.023*\"sign\" + '\n",
      "  '0.022*\"help\" + 0.021*\"referendum\" + 0.020*\"today\" + 0.019*\"enough\" + '\n",
      "  '0.019*\"change\"'),\n",
      " (2,\n",
      "  '0.055*\"vote\" + 0.051*\"deal\" + 0.034*\"leave\" + 0.032*\"tory\" + 0.026*\"people\" '\n",
      "  '+ 0.025*\"stop\" + 0.022*\"labour\" + 0.021*\"good\" + 0.021*\"could\" + '\n",
      "  '0.020*\"party\"'),\n",
      " (3,\n",
      "  '0.046*\"come\" + 0.036*\"support\" + 0.034*\"fake\" + 0.033*\"believe\" + '\n",
      "  '0.032*\"plan\" + 0.030*\"money\" + 0.028*\"thing\" + 0.026*\"well\" + 0.022*\"fail\" '\n",
      "  '+ 0.022*\"win\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model_n.print_topics())\n",
    "doc_lda_n = lda_model_n[corpus_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: say|election|conservative|need|corbyn|make|deliver|remain|campaign|farage\n",
      "Topic: 1 \n",
      "Words: keep|full|fact|call|sign|help|referendum|today|enough|change\n",
      "Topic: 2 \n",
      "Words: vote|deal|leave|tory|people|stop|labour|good|could|party\n",
      "Topic: 3 \n",
      "Words: come|support|fake|believe|plan|money|thing|well|fail|win\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_n.show_topics(formatted=False, num_words= 10):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Coherence Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.42304886230384486\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_n, texts=data_lemmatized_n, dictionary=id2word_n, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of the Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el158885997041034965353874960\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el158885997041034965353874960_data = {\"mdsDat\": {\"x\": [-0.35622532922034883, 0.11029026222411673, 0.3426463991060462, -0.09671133210981413], \"y\": [0.09995295859234768, 0.34059400629224224, -0.11495714821679891, -0.32558981666779097], \"topics\": [1, 2, 3, 4], \"cluster\": [1, 1, 1, 1], \"Freq\": [38.768653869628906, 21.9959659576416, 21.684532165527344, 17.550851821899414]}, \"tinfo\": {\"Term\": [\"vote\", \"polling\", \"claim\", \"tory\", \"conservative\", \"remain\", \"brexit\", \"talk\", \"labour\", \"could\", \"deal\", \"delay\", \"election\", \"need\", \"leader\", \"face\", \"say\", \"people\", \"sign\", \"leave\", \"happen\", \"today\", \"lie\", \"good\", \"government\", \"make\", \"help\", \"deliver\", \"party\", \"month\", \"conservative\", \"brexit\", \"deal\", \"say\", \"people\", \"leave\", \"make\", \"deliver\", \"party\", \"time\", \"support\", \"come\", \"thank\", \"trade\", \"agree\", \"look\", \"country\", \"must\", \"stop\", \"open\", \"letter\", \"voter\", \"show\", \"change\", \"bad\", \"plan\", \"referendum\", \"work\", \"last\", \"word\", \"force\", \"vote\", \"remain\", \"labour\", \"need\", \"face\", \"today\", \"good\", \"government\", \"call\", \"lose\", \"read\", \"polling\", \"break\", \"british\", \"demand\", \"warn\", \"democracy\", \"well\", \"public\", \"much\", \"confidence\", \"trust\", \"blame\", \"reach\", \"woman\", \"agreement\", \"decision\", \"extension\", \"hold\", \"part\", \"exit\", \"abandon\", \"trash\", \"tory\", \"claim\", \"could\", \"election\", \"leader\", \"month\", \"corbyn\", \"news\", \"right\", \"stay\", \"become\", \"parliament\", \"poor\", \"keep\", \"stand\", \"member\", \"speak\", \"seem\", \"sell\", \"place\", \"world\", \"report\", \"listen\", \"enough\", \"foreign\", \"negotiation\", \"allow\", \"pass\", \"responsible\", \"care\", \"philiphammonduk\", \"scenario\", \"mother\", \"talk\", \"delay\", \"sign\", \"happen\", \"lie\", \"help\", \"turn\", \"promise\", \"hope\", \"destroy\", \"fail\", \"control\", \"hear\", \"leadership\", \"risk\", \"great\", \"back\", \"politic\", \"start\", \"chance\", \"reject\", \"decide\", \"correct\", \"remove\", \"brand\", \"term\", \"downingstreet\", \"problem\", \"live\", \"replace\", \"ignore\", \"spout\", \"earth\", \"language\"], \"Freq\": [197.0, 166.0, 151.0, 140.0, 193.0, 117.0, 187.0, 97.0, 112.0, 108.0, 172.0, 87.0, 91.0, 90.0, 87.0, 86.0, 141.0, 140.0, 69.0, 131.0, 68.0, 74.0, 63.0, 72.0, 71.0, 117.0, 59.0, 114.0, 111.0, 66.0, 192.81834411621094, 187.15171813964844, 171.90013122558594, 140.98313903808594, 139.4655303955078, 130.66876220703125, 116.59317779541016, 113.57620239257812, 110.66193389892578, 101.55159759521484, 92.83232879638672, 84.66813659667969, 78.28489685058594, 78.34758758544922, 76.25934600830078, 72.02433776855469, 68.63463592529297, 68.23954772949219, 65.12198638916016, 65.90235900878906, 69.11602020263672, 59.322757720947266, 57.96053695678711, 54.490020751953125, 53.93205642700195, 52.386016845703125, 53.27650451660156, 51.9633903503418, 51.901729583740234, 53.05954360961914, 53.490543365478516, 196.84408569335938, 116.424072265625, 111.32560729980469, 89.3656234741211, 85.65129089355469, 74.17761993408203, 71.38861846923828, 70.98130798339844, 63.869327545166016, 62.486392974853516, 57.034481048583984, 163.5802001953125, 49.531715393066406, 44.15966033935547, 42.79353713989258, 40.42686462402344, 37.74175262451172, 37.469486236572266, 34.51345443725586, 34.66165542602539, 32.737327575683594, 32.328556060791016, 31.103038787841797, 33.51042556762695, 32.08460998535156, 30.7559871673584, 30.144474029541016, 31.40841293334961, 28.547565460205078, 27.979961395263672, 31.128726959228516, 29.985153198242188, 33.145076751708984, 139.6911163330078, 150.6834259033203, 107.83331298828125, 90.34494018554688, 86.71560668945312, 65.44538879394531, 59.12999725341797, 57.67464065551758, 51.523414611816406, 52.49515914916992, 49.101409912109375, 47.28661346435547, 44.370548248291016, 39.0933837890625, 38.77385330200195, 38.137168884277344, 36.69170379638672, 33.852500915527344, 31.938505172729492, 34.30657196044922, 31.618234634399414, 31.77249526977539, 30.47012710571289, 29.74517250061035, 40.49061965942383, 32.048709869384766, 28.31859016418457, 28.498445510864258, 42.953311920166016, 26.46696662902832, 37.175079345703125, 36.52142333984375, 28.811500549316406, 96.25338745117188, 86.56739044189453, 68.95524597167969, 68.01895904541016, 62.572330474853516, 59.109466552734375, 52.31142044067383, 50.88943099975586, 47.832298278808594, 44.98392105102539, 43.42100524902344, 48.64107131958008, 41.06172561645508, 39.87443161010742, 41.29865646362305, 38.076942443847656, 35.34137725830078, 31.826440811157227, 31.949438095092773, 27.970958709716797, 26.392112731933594, 25.13661766052246, 28.191022872924805, 26.168058395385742, 26.038667678833008, 24.619348526000977, 24.006338119506836, 23.73719596862793, 23.321746826171875, 25.071746826171875, 23.88560676574707, 31.64272689819336, 31.227073669433594, 25.881532669067383], \"Total\": [197.0, 166.0, 151.0, 140.0, 193.0, 117.0, 187.0, 97.0, 112.0, 108.0, 172.0, 87.0, 91.0, 90.0, 87.0, 86.0, 141.0, 140.0, 69.0, 131.0, 68.0, 74.0, 63.0, 72.0, 71.0, 117.0, 59.0, 114.0, 111.0, 66.0, 193.59384155273438, 187.92955017089844, 172.6524200439453, 141.74478149414062, 140.22142028808594, 131.42120361328125, 117.35038757324219, 114.35932922363281, 111.42764282226562, 102.30731201171875, 93.595947265625, 85.43436431884766, 79.05555725097656, 79.13208770751953, 77.03539276123047, 72.77655029296875, 69.38172912597656, 68.99424743652344, 65.87633514404297, 66.70106506347656, 69.97906494140625, 60.09541320800781, 58.72225570678711, 55.235328674316406, 54.695133209228516, 53.12823486328125, 54.033599853515625, 52.70716094970703, 52.66811752319336, 53.847801208496094, 54.291343688964844, 197.60604858398438, 117.2177505493164, 112.10860443115234, 90.13733673095703, 86.44265747070312, 74.95805358886719, 72.14826202392578, 71.75114440917969, 64.63729095458984, 63.275020599365234, 57.86159133911133, 166.05014038085938, 50.34112548828125, 44.915836334228516, 43.57330322265625, 41.2080192565918, 38.507198333740234, 38.244163513183594, 35.270301818847656, 35.43049621582031, 33.50187683105469, 33.10253143310547, 31.851913452148438, 34.32811737060547, 32.871673583984375, 31.527677536010742, 30.914722442626953, 32.21620178222656, 29.312829971313477, 28.74272918701172, 31.988317489624023, 30.87000274658203, 34.35986328125, 140.4742431640625, 151.6491241455078, 108.60921478271484, 91.12060546875, 87.50540924072266, 66.26748657226562, 59.90056610107422, 58.44956588745117, 52.28868103027344, 53.31511688232422, 49.875335693359375, 48.04808807373047, 45.18879318237305, 39.84700393676758, 39.54950714111328, 38.92162322998047, 37.522010803222656, 34.62438201904297, 32.7006950378418, 35.131717681884766, 32.40753936767578, 32.56988525390625, 31.238605499267578, 30.51967430114746, 41.556270599365234, 32.899925231933594, 29.090253829956055, 29.279809951782227, 44.188838958740234, 27.233247756958008, 38.30059051513672, 38.30662536621094, 29.72792625427246, 97.0208740234375, 87.40528106689453, 69.71403503417969, 68.77430725097656, 63.334678649902344, 59.867427825927734, 53.09589767456055, 51.655418395996094, 48.61077117919922, 45.7577018737793, 44.17466354370117, 49.51043701171875, 41.83258056640625, 40.63782501220703, 42.11932373046875, 38.8361930847168, 36.08675003051758, 32.5783576965332, 32.706024169921875, 28.74155044555664, 27.12852668762207, 25.890777587890625, 29.047733306884766, 26.967798233032227, 26.852563858032227, 25.39647674560547, 24.77060317993164, 24.495513916015625, 24.07573890686035, 25.883071899414062, 24.661592483520508, 32.989322662353516, 32.54956817626953, 26.813940048217773], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.661799907684326, -3.691699981689453, -3.776700019836426, -3.9749999046325684, -3.98580002784729, -4.050899982452393, -4.164899826049805, -4.191100120544434, -4.217100143432617, -4.302999973297119, -4.3927998542785645, -4.484899997711182, -4.563199996948242, -4.562399864196777, -4.589399814605713, -4.646599769592285, -4.694799900054932, -4.7006001472473145, -4.747300148010254, -4.735400199890137, -4.68779993057251, -4.84060001373291, -4.863800048828125, -4.925600051879883, -4.9359002113342285, -4.965000152587891, -4.9481000900268555, -4.973100185394287, -4.9741997718811035, -4.952199935913086, -4.9440999031066895, -3.074399948120117, -3.599600076675415, -3.644399881362915, -3.8640999794006348, -3.906599998474121, -4.0503997802734375, -4.088699817657471, -4.094399929046631, -4.199999809265137, -4.22189998626709, -4.313199996948242, -3.259500026702881, -4.45419979095459, -4.568999767303467, -4.600399971008301, -4.657299995422363, -4.726099967956543, -4.73330020904541, -4.815499782562256, -4.811200141906738, -4.868299961090088, -4.880899906158447, -4.91949987411499, -4.84499979019165, -4.888500213623047, -4.930799961090088, -4.950799942016602, -4.909800052642822, -5.005300045013428, -5.025300025939941, -4.918700218200684, -4.956099987030029, -4.855899810791016, -3.40310001373291, -3.327399969100952, -3.6619999408721924, -3.838900089263916, -3.8798999786376953, -4.161399841308594, -4.262800216674805, -4.287799835205078, -4.4004998207092285, -4.381899833679199, -4.448699951171875, -4.486400127410889, -4.550000190734863, -4.676599979400635, -4.684800148010254, -4.701399803161621, -4.739999771118164, -4.8206000328063965, -4.878799915313721, -4.807199954986572, -4.888800144195557, -4.883999824523926, -4.92579984664917, -4.949900150299072, -4.641499996185303, -4.87529993057251, -4.999100208282471, -4.992700099945068, -4.582499980926514, -5.066699981689453, -4.726900100708008, -4.744699954986572, -4.981800079345703, -3.5641000270843506, -3.6702001094818115, -3.897599935531616, -3.91129994392395, -3.994800090789795, -4.051700115203857, -4.173900127410889, -4.201399803161621, -4.263400077819824, -4.32480001449585, -4.360099792480469, -4.246600151062012, -4.415999889373779, -4.445300102233887, -4.410299777984619, -4.491499900817871, -4.565999984741211, -4.67080020904541, -4.666900157928467, -4.799900054931641, -4.857999801635742, -4.906799793243408, -4.792099952697754, -4.866499900817871, -4.871500015258789, -4.927499771118164, -4.9527997970581055, -4.964000225067139, -4.9816999435424805, -4.909299850463867, -4.957799911499023, -4.676599979400635, -4.689799785614014, -4.877600193023682], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9434999823570251, 0.9434000253677368, 0.9431999921798706, 0.9422000050544739, 0.9422000050544739, 0.9417999982833862, 0.941100001335144, 0.9406999945640564, 0.9406999945640564, 0.9401000142097473, 0.9394000172615051, 0.9384999871253967, 0.9377999901771545, 0.9376000165939331, 0.9373999834060669, 0.9372000098228455, 0.9366999864578247, 0.9366000294685364, 0.9359999895095825, 0.9355000257492065, 0.9351000189781189, 0.9345999956130981, 0.934499979019165, 0.9340000152587891, 0.9334999918937683, 0.9334999918937683, 0.9333999752998352, 0.9333000183105469, 0.9329000115394592, 0.9327999949455261, 0.932699978351593, 1.5104000568389893, 1.5075000524520874, 1.5073000192642212, 1.5056999921798706, 1.5051000118255615, 1.5038000345230103, 1.5037000179290771, 1.503499984741211, 1.5024000406265259, 1.5017999410629272, 1.499899983406067, 1.4993000030517578, 1.4981000423431396, 1.4973000288009644, 1.4962999820709229, 1.4952000379562378, 1.4941999912261963, 1.4938000440597534, 1.4925999641418457, 1.492400050163269, 1.4911999702453613, 1.4907000064849854, 1.4904999732971191, 1.4902000427246094, 1.4901000261306763, 1.4895000457763672, 1.4890999794006348, 1.4888999462127686, 1.4879000186920166, 1.4874000549316406, 1.4871000051498413, 1.485200047492981, 1.4782999753952026, 1.5230000019073486, 1.5221999883651733, 1.521399974822998, 1.5199999809265137, 1.5195000171661377, 1.5161000490188599, 1.5155999660491943, 1.5152000188827515, 1.513800024986267, 1.513100028038025, 1.5128999948501587, 1.5125999450683594, 1.5103000402450562, 1.5095000267028809, 1.5088000297546387, 1.5082000494003296, 1.5061999559402466, 1.50600004196167, 1.5049999952316284, 1.5047999620437622, 1.5039000511169434, 1.5038000345230103, 1.5037000179290771, 1.5029000043869019, 1.5025999546051025, 1.5024000406265259, 1.5017000436782837, 1.5015000104904175, 1.5002000331878662, 1.5, 1.4987000226974487, 1.4808000326156616, 1.4973000288009644, 1.732100009918213, 1.7303999662399292, 1.729099988937378, 1.7289999723434448, 1.7280000448226929, 1.7273000478744507, 1.7252000570297241, 1.725100040435791, 1.7238999605178833, 1.7230000495910645, 1.7229000329971313, 1.7223999500274658, 1.721500039100647, 1.7210999727249146, 1.7203999757766724, 1.7202999591827393, 1.7192000150680542, 1.7166999578475952, 1.7166999578475952, 1.7129000425338745, 1.712499976158142, 1.7105000019073486, 1.7101000547409058, 1.7100000381469727, 1.7093000411987305, 1.7089999914169312, 1.7086999416351318, 1.7086000442504883, 1.7081999778747559, 1.7081999778747559, 1.7080999612808228, 1.6984000205993652, 1.6986000537872314, 1.704699993133545]}, \"token.table\": {\"Topic\": [2, 1, 2, 3, 4, 1, 3, 2, 4, 2, 1, 2, 2, 3, 4, 1, 3, 1, 2, 1, 4, 3, 4, 3, 1, 1, 4, 2, 4, 1, 2, 2, 4, 4, 4, 3, 3, 2, 2, 2, 4, 1, 3, 2, 2, 4, 4, 4, 4, 2, 4, 4, 3, 2, 4, 1, 3, 4, 1, 1, 4, 3, 4, 1, 2, 1, 3, 3, 3, 2, 1, 2, 3, 3, 1, 3, 2, 1, 3, 1, 3, 3, 1, 4, 1, 2, 3, 4, 3, 4, 4, 2, 2, 2, 1, 4, 2, 4, 4, 3, 3, 3, 4, 1, 1, 2, 3, 4, 3, 3, 1, 4, 3, 4, 3, 4, 3, 1, 1, 4, 4, 1, 1, 2, 3, 1, 2, 2, 4, 2, 1, 2, 2, 2, 1, 1, 3], \"Freq\": [0.9718171954154968, 0.9865595102310181, 0.9832630157470703, 0.9625216722488403, 0.9698850512504578, 0.9872907400131226, 0.9824495315551758, 0.9732539057731628, 0.9682502150535583, 0.9932237267494202, 0.9950537085533142, 0.9796099662780762, 0.9901404976844788, 0.954715371131897, 0.9741993546485901, 0.9776351451873779, 0.9957195520401001, 0.994915783405304, 0.9850194454193115, 0.9969325661659241, 0.9896903038024902, 0.9849656224250793, 0.963930606842041, 0.9943907856941223, 0.9944981336593628, 0.9962211847305298, 0.9655947685241699, 0.9704114198684692, 0.995363175868988, 0.9968578815460205, 0.9868428111076355, 0.9868285059928894, 0.9834409952163696, 0.9688904285430908, 0.9523935914039612, 0.987701952457428, 0.9829725027084351, 0.9691038131713867, 0.9622487425804138, 0.9948791861534119, 0.9734086394309998, 0.9762145280838013, 0.9625502824783325, 0.9840846657752991, 0.9895312786102295, 0.9784687161445618, 0.9887413382530212, 0.9800972938537598, 0.9855108261108398, 0.9893279075622559, 0.9874354600906372, 0.9731731414794922, 0.9787436127662659, 0.9901113510131836, 0.9696449041366577, 0.9873145818710327, 0.9942242503166199, 0.984304666519165, 0.9967949986457825, 0.9860091805458069, 0.994715690612793, 0.9603501558303833, 0.9553185701370239, 0.9893296360969543, 0.9798495173454285, 0.9970141649246216, 0.9763210415840149, 0.980873167514801, 0.9755136966705322, 0.9878495335578918, 0.9855894446372986, 0.987382173538208, 0.9726465940475464, 0.992308497428894, 0.9894894361495972, 0.9781866669654846, 0.9741594195365906, 0.9961621761322021, 0.9562903642654419, 0.9912893176078796, 0.9660425186157227, 0.967786431312561, 0.9787639379501343, 0.9822471737861633, 0.006022277288138866, 0.9876534938812256, 0.006022277288138866, 0.006022277288138866, 0.9736927151679993, 0.9797712564468384, 0.9873117208480835, 0.9923362731933594, 0.9904417395591736, 0.9851094484329224, 0.9808711409568787, 0.9584007263183594, 0.9896112084388733, 0.9641128182411194, 0.9658822417259216, 0.9825026988983154, 0.9730963706970215, 0.9944791197776794, 0.9734249114990234, 0.9947456121444702, 0.02610514499247074, 0.02610514499247074, 0.9658903479576111, 0.02610514499247074, 0.9819669723510742, 0.9785724878311157, 0.9877004623413086, 0.9897576570510864, 0.9860878586769104, 0.9700108170509338, 0.9861058592796326, 0.9784130454063416, 0.9753331542015076, 0.986697256565094, 0.9936327934265137, 0.989477813243866, 0.984388530254364, 0.98664790391922, 0.9969961643218994, 0.9872187972068787, 0.9966239929199219, 0.9856936931610107, 0.960422933101654, 0.9666934609413147, 0.979360044002533, 0.9969330430030823, 0.9817721247673035, 0.970684826374054, 0.9674679040908813, 0.9734825491905212, 0.9842556118965149, 0.9865832328796387, 0.9874245524406433], \"Term\": [\"abandon\", \"agree\", \"agreement\", \"allow\", \"back\", \"bad\", \"become\", \"blame\", \"brand\", \"break\", \"brexit\", \"british\", \"call\", \"care\", \"chance\", \"change\", \"claim\", \"come\", \"confidence\", \"conservative\", \"control\", \"corbyn\", \"correct\", \"could\", \"country\", \"deal\", \"decide\", \"decision\", \"delay\", \"deliver\", \"demand\", \"democracy\", \"destroy\", \"downingstreet\", \"earth\", \"election\", \"enough\", \"exit\", \"extension\", \"face\", \"fail\", \"force\", \"foreign\", \"good\", \"government\", \"great\", \"happen\", \"hear\", \"help\", \"hold\", \"hope\", \"ignore\", \"keep\", \"labour\", \"language\", \"last\", \"leader\", \"leadership\", \"leave\", \"letter\", \"lie\", \"listen\", \"live\", \"look\", \"lose\", \"make\", \"member\", \"month\", \"mother\", \"much\", \"must\", \"need\", \"negotiation\", \"news\", \"open\", \"parliament\", \"part\", \"party\", \"pass\", \"people\", \"philiphammonduk\", \"place\", \"plan\", \"politic\", \"polling\", \"polling\", \"polling\", \"polling\", \"poor\", \"problem\", \"promise\", \"public\", \"reach\", \"read\", \"referendum\", \"reject\", \"remain\", \"remove\", \"replace\", \"report\", \"responsible\", \"right\", \"risk\", \"say\", \"scenario\", \"scenario\", \"scenario\", \"scenario\", \"seem\", \"sell\", \"show\", \"sign\", \"speak\", \"spout\", \"stand\", \"start\", \"stay\", \"stop\", \"support\", \"talk\", \"term\", \"thank\", \"time\", \"today\", \"tory\", \"trade\", \"trash\", \"trust\", \"turn\", \"vote\", \"voter\", \"warn\", \"well\", \"woman\", \"word\", \"work\", \"world\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 2, 3, 1]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el158885997041034965353874960\", ldavis_el158885997041034965353874960_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el158885997041034965353874960\", ldavis_el158885997041034965353874960_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el158885997041034965353874960\", ldavis_el158885997041034965353874960_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_data =  pyLDAvis.gensim.prepare(lda_model_m, tfidf_corpus_m, id2word_m, mds='mmds')\n",
    "pyLDAvis.display(lda_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
