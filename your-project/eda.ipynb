{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(s):\n",
    "    words = s.split()\n",
    "    words = [w.lower() for w in words if not w.startswith('http://')]\n",
    "    words = re.findall('[^\\d\\W]+', ' '.join(words))\n",
    "    return ' '.join(words)\n",
    "\n",
    "def tokenize(s):\n",
    "    return word_tokenize(s)\n",
    "\n",
    "def stem_and_lemmatize(l):\n",
    "    l = ' '.join(l)\n",
    "    stem = PorterStemmer().stem(l)\n",
    "    lemm = WordNetLemmatizer().lemmatize(stem)\n",
    "    return lemm\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    stop_words = stopwords.words('english')\n",
    "    return [i for i in l.split() if i not in stop_words]\n",
    "\n",
    "def get_pm(row):\n",
    "    pms = []\n",
    "    text = row[\"text\"].lower()\n",
    "    if \"boris\" in text or \"johnson\" in text:\n",
    "        pms.append(\"boris\")\n",
    "    if \"theresa\" in text:\n",
    "        pms.append(\"may\")\n",
    "    else:\n",
    "        pms.append(\"none\") \n",
    "    return \",\".join(pms)\n",
    "\n",
    "def sentiment_nlkt(text):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    polarity_scores = sid.polarity_scores(text)\n",
    "    return 'neg' if polarity_scores['neg'] > polarity_scores['pos'] else 'pos'\n",
    "\n",
    "def sentiment_textblob(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return 'pos' if analysis.sentiment.polarity >= 0 else 'neg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'/Users/ironhack/Documents/GitHub/IronHack/W9FinalProject/final-project/your-project/tweets/' # use your path\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "\n",
    "df = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = frame[['timestamp','tweet_id','tweet_text']]\n",
    "frame = frame.rename(columns={'timestamp': 'date', 'tweet_id': 'id', 'tweet_text': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()*100/len(df161)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['date','id', 'username', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing time type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(result['date'])\n",
    "df['date'] = [d.date() for d in result['date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating column for Theresa May/Boris Jonhson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pm\"] = df.apply(get_pm,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "df[\"pm_label\"] = le.fit_transform(df.pm.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pm\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['pm'] == 'may']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_processed'] = df['text'].apply(clean_up).apply(tokenize).apply(stem_and_lemmatize)\\\n",
    "                       .apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words = 'english')\n",
    "words = cv.fit_transform(df['text'])\n",
    "\n",
    "#df161['text'].apply(lambda x: cv.fit_transform(x))\n",
    "\n",
    "sum_words = words.sum(axis=0)\n",
    "\n",
    "words_freq = [(word, sum_words[0, i]) for word, i in cv.vocabulary_.items()]\n",
    "words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n",
    "\n",
    "frequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n",
    "\n",
    "frequency.head(30).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = 'blue')\n",
    "plt.title(\"Most Frequently Occuring Words - Top 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countVectorizer = CountVectorizer(analyzer=clean_text) \n",
    "countVector = countVectorizer.fit_transform(df['text'])\n",
    "print('{} Number of tweets has {} words'.format(countVector.shape[0], countVector.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = [w for words in df['text_processed'] for w in words if len(w) > 1]\n",
    "bow = {k: total_words.count(k) for k in total_words}\n",
    "sorted_bow = sorted(bow.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = {k: v for k,v in sorted_bow[:50]}\n",
    "words = pd.DataFrame(sb, index=['values'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, n_init = 20, n_jobs = 1) # n_init(number of iterations for clsutering) n_jobs(number of cpu cores to use)\n",
    "kmeans.fit(df161['text'])\n",
    "# We look at 3 the clusters generated by k-means.\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking sentiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with nlkt sentiment analysys\n",
    "sentiments_nlkt = df['processed_text'].apply(lambda tweet: sentiment_nlkt(tweet))\n",
    "pd.DataFrame(sentiments_nlkt.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with textblob sentiment analysys\n",
    "sentiments_textblob = df['processed_text'].apply(lambda tweet: sentiment_textblob(tweet))\n",
    "pd.DataFrame(sentiments_textblob.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
